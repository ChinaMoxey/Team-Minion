{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Commands to install required packages\n",
    "#!pip install tweepy==3.3.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#part 1\n",
    "import tweepy\n",
    "from tweepy import OAuthHandler\n",
    " \n",
    "consumer_key = 'YOUR-CONSUMER-KEY'\n",
    "consumer_secret = 'YOUR-CONSUMER-SECRET'\n",
    "access_token = 'YOUR-ACCESS-TOKEN'\n",
    "access_secret = 'YOUR-ACCESS-SECRET'\n",
    " \n",
    "auth = OAuthHandler(consumer_key, consumer_secret)\n",
    "auth.set_access_token(access_token, access_secret)\n",
    " \n",
    "api = tweepy.API(auth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define process or store\n",
    "def process_or_store(tweet):\n",
    "    print(json.dumps(tweet))\n",
    "\n",
    "for status in tweepy.Cursor(api.home_timeline).items(10):\n",
    "    # Process a single status\n",
    "    process_or_store(status._json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tweepy import Stream\n",
    "from tweepy.streaming import StreamListener\n",
    " \n",
    "class MyListener(StreamListener):\n",
    " \n",
    "    def on_data(self, data):\n",
    "        try:\n",
    "            with open('python.json', 'a') as f:\n",
    "                f.write(data)\n",
    "                return True\n",
    "        except BaseException as e:\n",
    "            print(\"Error on_data: %s\" % str(e))\n",
    "        return True\n",
    " \n",
    "    def on_error(self, status):\n",
    "        print(status)\n",
    "        return True\n",
    " \n",
    "twitter_stream = Stream(auth, MyListener())\n",
    "twitter_stream.filter(track=['#python'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 2\n",
    "#After storing tweets as a JSON\n",
    "import json\n",
    "\n",
    "with open('mytweets.json', 'r') as f:\n",
    "    line = f.readline() # read only the first tweet/line\n",
    "    tweet = json.loads(line) # load it as Python dict\n",
    "    print(json.dumps(tweet, indent=4)) # pretty-print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    " \n",
    "tweet = 'RT @marcobonzanini: just an example! :D http://example.com #NLP'\n",
    "print(word_tokenize(tweet))\n",
    "# ['RT', '@', 'marcobonzanini', ':', 'just', 'an', 'example', '!', ':', 'D', 'http', ':', '//example.com', '#', 'NLP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Text Preprocessing\n",
    "import re\n",
    " \n",
    "emoticons_str = r\"\"\"\n",
    "    (?:\n",
    "        [:=;] # Eyes\n",
    "        [oO\\-]? # Nose (optional)\n",
    "        [D\\)\\]\\(\\]/\\\\OpP] # Mouth\n",
    "    )\"\"\"\n",
    " \n",
    "regex_str = [\n",
    "    emoticons_str,\n",
    "    r'<[^>]+>', # HTML tags\n",
    "    r'(?:@[\\w_]+)', # @-mentions\n",
    "    r\"(?:\\#+[\\w_]+[\\w\\'_\\-]*[\\w_]+)\", # hash-tags\n",
    "    r'http[s]?://(?:[a-z]|[0-9]|[$-_@.&amp;+]|[!*\\(\\),]|(?:%[0-9a-f][0-9a-f]))+', # URLs\n",
    " \n",
    "    r'(?:(?:\\d+,?)+(?:\\.?\\d+)?)', # numbers\n",
    "    r\"(?:[a-z][a-z'\\-_]+[a-z])\", # words with - and '\n",
    "    r'(?:[\\w_]+)', # other words\n",
    "    r'(?:\\S)' # anything else\n",
    "]\n",
    "    \n",
    "tokens_re = re.compile(r'('+'|'.join(regex_str)+')', re.VERBOSE | re.IGNORECASE)\n",
    "emoticon_re = re.compile(r'^'+emoticons_str+'$', re.VERBOSE | re.IGNORECASE)\n",
    " \n",
    "def tokenize(s):\n",
    "    return tokens_re.findall(s)\n",
    " \n",
    "def preprocess(s, lowercase=False):\n",
    "    tokens = tokenize(s)\n",
    "    if lowercase:\n",
    "        tokens = [token if emoticon_re.search(token) else token.lower() for token in tokens]\n",
    "    return tokens\n",
    " \n",
    "tweet = 'RT @marcobonzanini: just an example! :D http://example.com #NLP'\n",
    "print(preprocess(tweet))\n",
    "# ['RT', '@marcobonzanini', ':', 'just', 'an', 'example', '!', ':D', 'http://example.com', '#NLP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#process all the tweets do something needs to be defined\n",
    "def do_something_else(token):\n",
    "    print(json.dumps(token))\n",
    "\n",
    "with open('mytweets.json', 'r') as f:\n",
    "    for line in f:\n",
    "        tweet = json.loads(line)\n",
    "        tokens = preprocess(tweet['text'])\n",
    "        do_something_else(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 3 count terms-basic, first iteration\n",
    "import operator \n",
    "from collections import Counter\n",
    " \n",
    "fname = 'mytweets.json'\n",
    "with open(fname, 'r') as f:\n",
    "    \n",
    "    count_all = Counter()\n",
    "    for line in f:\n",
    "        tweet = json.loads(line)\n",
    "        # Create a list with all the terms\n",
    "        terms_all = [term for term in preprocess(tweet['text'])]\n",
    "        # Update the counter\n",
    "        count_all.update(terms_all)\n",
    "    # Print the first 5 most frequent words\n",
    "    print(count_all.most_common(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find stop words\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    " \n",
    "punctuation = list(string.punctuation)\n",
    "stop = stopwords.words('english') + punctuation + ['rt', 'via']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Same as two cells up but with terms_stop instead of terms_all \n",
    "fname = 'mytweets.json'\n",
    "with open(fname, 'r') as f:\n",
    "    \n",
    "    count_all = Counter()\n",
    "    for line in f:\n",
    "        tweet = json.loads(line)\n",
    "        # Create a list with all the terms\n",
    "        terms_stop = [term for term in preprocess(tweet['text']) if term not in stop]        # Update the counter\n",
    "        count_all.update(terms_stop)\n",
    "    # Print the first 5 most frequent words\n",
    "    print(count_all.most_common(5))from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #Customize terms and tokens\n",
    "fname = 'mytweets.json'\n",
    "with open(fname, 'r') as f:\n",
    "    \n",
    "    count_all = Counter()\n",
    "    for line in f:\n",
    "        tweet = json.loads(line)\n",
    "        # Create a list with all the terms\n",
    "        terms_stop = [term for term in preprocess(tweet['text']) if term not in stop]        # Update the counter\n",
    "        count_all.update(terms_stop)\n",
    "    # Print the first 5 most frequent words\n",
    "    print(count_all.most_common(5))\n",
    "    # Count terms only once, equivalent to Document Frequency\n",
    "terms_single = set(terms_all)\n",
    "# Count hashtags only\n",
    "terms_hash = [term for term in preprocess(tweet['text']) \n",
    "              if term.startswith('#')]\n",
    "# Count terms only (no hashtags, no mentions)\n",
    "terms_only = [term for term in preprocess(tweet['text']) \n",
    "              if term not in stop and\n",
    "              not term.startswith(('#', '@'))] \n",
    "              # mind the ((double brackets))\n",
    "              # startswith() takes a tuple (not a list) if \n",
    "              # we pass a list of inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#refactor of co-occurence matrix\n",
    "from collections import defaultdict\n",
    "# remember to include the other import from the previous post\n",
    " \n",
    "com = defaultdict(lambda : defaultdict(int))\n",
    " \n",
    "# f is the file pointer to the JSON data set\n",
    "for line in f: \n",
    "    tweet = json.loads(line)\n",
    "    terms_only = [term for term in preprocess(tweet['text']) \n",
    "                  if term not in stop \n",
    "                  and not term.startswith(('#', '@'))]\n",
    " \n",
    "    # Build co-occurrence matrix\n",
    "    for i in range(len(terms_only)-1):            \n",
    "        for j in range(i+1, len(terms_only)):\n",
    "            w1, w2 = sorted([terms_only[i], terms_only[j]])                \n",
    "            if w1 != w2:\n",
    "                com[w1][w2] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "com_max = []\n",
    "# For each term, look for the most common co-occurrent terms\n",
    "for t1 in com:\n",
    "    t1_max_terms = sorted(com[t1].items(), key=operator.itemgetter(1), reverse=True)[:5]\n",
    "    for t2, t2_count in t1_max_terms:\n",
    "        com_max.append(((t1, t2), t2_count))\n",
    "# Get the most frequent co-occurrences\n",
    "terms_max = sorted(com_max, key=operator.itemgetter(1), reverse=True)\n",
    "print(terms_max[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_word = sys.argv[1] # pass a term as a command-line argument\n",
    "count_search = Counter()\n",
    "for line in f:\n",
    "    tweet = json.loads(line)\n",
    "    terms_only = [term for term in preprocess(tweet['text']) \n",
    "                  if term not in stop \n",
    "                  and not term.startswith(('#', '@'))]\n",
    "    if search_word in terms_only:\n",
    "        count_search.update(terms_only)\n",
    "print(\"Co-occurrence for %s:\" % search_word)\n",
    "print(count_search.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part 5 deals with Vincent, which is deprecated. Will therefore not copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "\n",
    " \n",
    "dates_ITAvWAL = []\n",
    "# f is the file pointer to the JSON data set\n",
    "for line in f:\n",
    "    tweet = json.loads(line)\n",
    "    # let's focus on hashtags only at the moment\n",
    "    terms_hash = [term for term in preprocess(tweet['text']) if term.startswith('#')]\n",
    "    # track when the hashtag is mentioned\n",
    "    if '#itavwal' in terms_hash:\n",
    "        dates_ITAvWAL.append(tweet['created_at'])\n",
    " \n",
    "# a list of \"1\" to count the hashtags\n",
    "ones = [1]*len(dates_ITAvWAL)\n",
    "# the index of the series\n",
    "idx = pandas.DatetimeIndex(dates_ITAvWAL)\n",
    "# the actual series (at series of 1s for the moment)\n",
    "ITAvWAL = pandas.Series(ones, index=idx)\n",
    " \n",
    "# Resampling / bucketing\n",
    "per_minute = ITAvWAL.resample('1Min', how='sum').fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Positive and Negative Terms\n",
    "positive_vocab = [\n",
    "    'good', 'nice', 'great', 'awesome', 'outstanding',\n",
    "    'fantastic', 'terrific', ':)', ':-)', 'like', 'love',\n",
    "    # shall we also include game-specific terms?\n",
    "    # 'triumph', 'triumphal', 'triumphant', 'victory', etc.\n",
    "]\n",
    "negative_vocab = [\n",
    "    'bad', 'terrible', 'crap', 'useless', 'hate', ':(', ':-(',\n",
    "    # 'defeat', etc.\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pmi = defaultdict(lambda : defaultdict(int))\n",
    "for t1 in p_t:\n",
    "    for t2 in com[t1]:\n",
    "        denom = p_t[t1] * p_t[t2]\n",
    "        pmi[t1][t2] = math.log2(p_t_com[t1][t2] / denom)\n",
    " \n",
    "semantic_orientation = {}\n",
    "for term, n in p_t.items():\n",
    "    positive_assoc = sum(pmi[term][tx] for tx in positive_vocab)\n",
    "    negative_assoc = sum(pmi[term][tx] for tx in negative_vocab)\n",
    "    semantic_orientation[term] = positive_assoc - negative_assoc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_sorted = sorted(semantic_orientation.items(), \n",
    "                         key=operator.itemgetter(1), \n",
    "                         reverse=True)\n",
    "top_pos = semantic_sorted[:10]\n",
    "top_neg = semantic_sorted[-10:]\n",
    " \n",
    "print(top_pos)\n",
    "print(top_neg)\n",
    "print(\"ITA v WAL: %f\" % semantic_orientation['#itavwal'])\n",
    "print(\"SCO v IRE: %f\" % semantic_orientation['#scovire'])\n",
    "print(\"ENG v FRA: %f\" % semantic_orientation['#engvfra'])\n",
    "print(\"#ITA: %f\" % semantic_orientation['#ita'])\n",
    "print(\"#FRA: %f\" % semantic_orientation['#fra'])\n",
    "print(\"#SCO: %f\" % semantic_orientation['#sco'])\n",
    "print(\"#ENG: %f\" % semantic_orientation['#eng'])\n",
    "print(\"#WAL: %f\" % semantic_orientation['#wal'])\n",
    "print(\"#IRE: %f\" % semantic_orientation['#ire'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tweets are stored in \"fname\"\n",
    "with open(fname, 'r') as f:\n",
    "    geo_data = {\n",
    "        \"type\": \"FeatureCollection\",\n",
    "        \"features\": []\n",
    "    }\n",
    "    for line in f:\n",
    "        tweet = json.loads(line)\n",
    "        if tweet['coordinates']:\n",
    "            geo_json_feature = {\n",
    "                \"type\": \"Feature\",\n",
    "                \"geometry\": tweet['coordinates'],\n",
    "                \"properties\": {\n",
    "                    \"text\": tweet['text'],\n",
    "                    \"created_at\": tweet['created_at']\n",
    "                }\n",
    "            }\n",
    "            geo_data['features'].append(geo_json_feature)\n",
    " \n",
    "# Save geo data\n",
    "with open('geo_data.json', 'w') as fout:\n",
    "    fout.write(json.dumps(geo_data, indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Leaflet.js to be put in head of document. also seems to be for html stuff\n",
    "<link rel=\"stylesheet\" href=\"http://cdnjs.cloudflare.com/ajax/libs/leaflet/0.7.3/leaflet.css\" />\n",
    "<script src=\"http://cdnjs.cloudflare.com/ajax/libs/leaflet/0.7.3/leaflet.js\"></script>\n",
    "<script src=\"http://code.jquery.com/jquery-2.1.0.min.js\"></script>\n",
    "<!-- this goes in the <head> -->\n",
    "<style>\n",
    "#map {\n",
    "    height: 600px;\n",
    "}\n",
    "</style>\n",
    "<!-- this goes in the <body> -->\n",
    "<div id=\"map\"></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "// Load the tile images from OpenStreetMap\n",
    "var mytiles = L.tileLayer('http://{s}.tile.osm.org/{z}/{x}/{y}.png', {\n",
    "    attribution: '&copy; <a href=\"http://osm.org/copyright\">OpenStreetMap</a> contributors'\n",
    "});\n",
    "// Initialise an empty map\n",
    "var map = L.map('map');\n",
    "// Read the GeoJSON data with jQuery, and create a circleMarker element for each tweet\n",
    "// Each tweet will be represented by a nice red dot\n",
    "$.getJSON(\"./geo_data.json\", function(data) {\n",
    "    var myStyle = {\n",
    "        radius: 2,\n",
    "        fillColor: \"red\",\n",
    "        color: \"red\",\n",
    "        weight: 1,\n",
    "        opacity: 1,\n",
    "        fillOpacity: 1\n",
    "    };\n",
    " \n",
    "    var geojson = L.geoJson(data, {\n",
    "        pointToLayer: function (feature, latlng) {\n",
    "            return L.circleMarker(latlng, myStyle);\n",
    "        }\n",
    "    });\n",
    "    geojson.addTo(map)\n",
    "});\n",
    "// Add the tiles to the map, and initialise the view in the middle of Europe\n",
    "map.addLayer(mytiles).setView([50.5, 5.0], 5);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
